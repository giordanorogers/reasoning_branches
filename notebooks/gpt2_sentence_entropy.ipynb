{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "llm = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Create an entropy calculation function\n",
    "def calc_entropy(probs, axis=1):\n",
    "    return entropy(probs, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[[ -36.2874,  -35.0114,  -38.0794,  ...,  -40.5164,  -41.3760,\n",
      "           -34.9193],\n",
      "         [ -80.1033,  -79.4853,  -83.6792,  ...,  -85.5055,  -84.5914,\n",
      "           -82.7390],\n",
      "         [ -82.4965,  -82.5529,  -85.9854,  ...,  -91.6856,  -89.3994,\n",
      "           -83.7149],\n",
      "         ...,\n",
      "         [-110.9339, -110.5447, -111.2388,  ..., -120.0823, -117.0155,\n",
      "          -110.3783],\n",
      "         [-127.2238, -128.5333, -133.2365,  ..., -136.7369, -132.4776,\n",
      "          -126.5492],\n",
      "         [ -81.8050,  -83.5660,  -84.7266,  ...,  -92.0885,  -88.4962,\n",
      "           -84.5517]]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Entropy: 1.2025204\n",
      "Token:  \n"
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "\n",
    "with llm.trace(\"The binary representation of the decimal number 170 is \"):\n",
    "\n",
    "    # Get the logits\n",
    "    logits = llm.lm_head.output.save()\n",
    "\n",
    "    # Convert logits to probs using softmax\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Convert to token\n",
    "    pred_token = logits.argmax(dim=-1).save()\n",
    "\n",
    "    # Calculate entropy on next token probs\n",
    "    pred_entropy = nnsight.apply(calc_entropy, probs.detach().cpu()).save()\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Entropy:\", pred_entropy[0][-1])\n",
    "print(\"Token:\", llm.tokenizer.decode(pred_token[0][-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nnsight\n",
    "from nnsight import LanguageModel\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load a language model\n",
    "llm = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\")\n",
    "\n",
    "# Function to calculate entropy from probabilities\n",
    "def calc_entropy(probs, axis=1):\n",
    "    return entropy(probs, axis=axis)\n",
    "\n",
    "# Prompt for generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# End of sentence tokens (period, question mark, exclamation point)\n",
    "end_tokens = [13, 30, 0, 986]  # . ? ! ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with llm.generate(prompt, max_new_tokens=100) as tracer:\n",
    "\n",
    "    # List to store entropy values and generated tokens\n",
    "    entropy_values = nnsight.list().save()\n",
    "    generated_tokens = nnsight.list().save()\n",
    "\n",
    "    entropy_values.append(1)\n",
    "\n",
    "print(entropy_values)\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'end_lineno'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate text until a complete sentence is formed\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mgenerate(prompt, max_new_tokens\u001b[38;5;241m=\u001b[39mn_new_tokens) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_new_tokens):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Save the logits\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         logits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m pred_embed \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert to token\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m pred_token \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate entropy\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pred_entropy \u001b[38;5;241m=\u001b[39m nnsight\u001b[38;5;241m.\u001b[39mapply(calc_entropy, probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3870\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3867\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3868\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    667\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 668\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    671\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/nnsight/tracing/graph/proxy.py:293\u001b[0m, in \u001b[0;36mProxy.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration control flow encountered but \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONFIG.APP.CONTROL_FLOW_HACKS\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is set to False\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterator\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_back\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/reasoning/lib/python3.10/site-packages/nnsight/tracing/hacks/iterator.py:103\u001b[0m, in \u001b[0;36mhandle_proxy\u001b[0;34m(frame, collection)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcallback\u001b[39m(iterator: Iterator):\n\u001b[1;32m    102\u001b[0m     iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m end \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_lineno \u001b[38;5;241m+\u001b[39m (\u001b[43mfor_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_lineno\u001b[49m \u001b[38;5;241m-\u001b[39m for_node\u001b[38;5;241m.\u001b[39mlineno)\n\u001b[1;32m    104\u001b[0m execute_until(frame\u001b[38;5;241m.\u001b[39mf_lineno, end, frame, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m _: callback(iterator))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([item])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'end_lineno'"
     ]
    }
   ],
   "source": [
    "n_new_tokens = 100\n",
    "\n",
    "# List to store entropy values and generated tokens\n",
    "entropy_values = []\n",
    "generated_tokens = []\n",
    "\n",
    "# Generate text until a complete sentence is formed\n",
    "with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "\n",
    "    with llm.lm_head.all():\n",
    "\n",
    "        # Save the logits\n",
    "        logits = llm.lm_head.output\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Get predicted token\n",
    "        pred_embed = logits.argmax(dim=-1)\n",
    "        # Decode token\n",
    "        pred_token = llm.tokenizer.decode(pred_embed).save()\n",
    "        # Calculate entropy\n",
    "        pred_entropy = nnsight.apply(calc_entropy, probs.detach().cpu()).save()\n",
    "\n",
    "        # Append values to our lists\n",
    "        entropy_values.append(pred_entropy)\n",
    "        generated_tokens.append(pred_token)\n",
    "\n",
    "        if pred_embed in end_tokens:\n",
    "            break\n",
    "\n",
    "print(generated_tokens)\n",
    "print(entropy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1849, device='mps:0'), tensor(40, device='mps:0')]\n",
      "2\n",
      "[tensor([[[8.9157e-04, 1.0424e-03, 2.1106e-05,  ..., 7.1022e-07,\n",
      "          4.8419e-06, 5.4752e-04],\n",
      "         [3.2892e-06, 1.9758e-05, 3.3276e-07,  ..., 3.1810e-09,\n",
      "          7.2668e-07, 3.3427e-06],\n",
      "         [4.0888e-07, 1.1701e-05, 1.7799e-07,  ..., 2.2446e-09,\n",
      "          1.6359e-06, 1.6848e-05],\n",
      "         [2.5378e-04, 1.3109e-04, 2.7051e-07,  ..., 2.2008e-07,\n",
      "          1.6496e-06, 2.9005e-05],\n",
      "         [8.0199e-06, 2.7697e-06, 2.6061e-07,  ..., 1.2774e-06,\n",
      "          1.4392e-06, 3.9997e-05],\n",
      "         [3.3102e-05, 6.4728e-05, 8.0429e-06,  ..., 1.4446e-08,\n",
      "          8.3535e-07, 2.2285e-06]]], device='mps:0'), tensor([[[1.1516e-05, 4.9644e-03, 1.3941e-04,  ..., 2.8901e-09,\n",
      "          1.7376e-09, 6.4813e-07]]], device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "# Old approach:\n",
    "prompt = 'Once upon a time, '\n",
    "layers = llm.transformer.h\n",
    "n_new_tokens = 30\n",
    "prob_values = []\n",
    "generated_tokens = []\n",
    "end_tokens = [243423]\n",
    "with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "\n",
    "    # First token\n",
    "    logits = llm.lm_head.output\n",
    "    probs = F.softmax(logits, dim=-1).save()\n",
    "    pred_token = logits.argmax(dim=-1)[0][-1].save()\n",
    "    prob_values.append(probs)\n",
    "    generated_tokens.append(pred_token)\n",
    "\n",
    "    # Subsequent tokens\n",
    "    for i in range(n_new_tokens-1):\n",
    "\n",
    "        # Save the logits\n",
    "        logits = llm.lm_head.next().output\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1).save()\n",
    "\n",
    "        # Get predicted token\n",
    "        pred_token = logits.argmax(dim=-1)[0][-1].save()\n",
    "\n",
    "        # Append\n",
    "        prob_values.append(probs)\n",
    "        generated_tokens.append(pred_token)\n",
    "\n",
    "        with tracer.cond(pred_token.item() == 13):\n",
    "            break\n",
    "\n",
    "\n",
    "print(generated_tokens)\n",
    "print(len(generated_tokens))\n",
    "print(prob_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n",
      " \n",
      "40\n",
      "I\n",
      "373\n",
      " was\n",
      "257\n",
      " a\n",
      "1310\n",
      " little\n",
      "1643\n",
      " bit\n",
      "286\n",
      " of\n",
      "257\n",
      " a\n",
      "4336\n",
      " fan\n",
      "286\n",
      " of\n",
      "262\n",
      " the\n",
      "2656\n",
      " original\n",
      "2168\n",
      " series\n",
      "11\n",
      ",\n",
      "475\n",
      " but\n",
      "314\n",
      " I\n",
      "373\n",
      " was\n",
      "635\n",
      " also\n",
      "257\n",
      " a\n",
      "1310\n",
      " little\n",
      "1643\n",
      " bit\n",
      "286\n",
      " of\n",
      "257\n",
      " a\n",
      "4336\n",
      " fan\n",
      "286\n",
      " of\n",
      "262\n",
      " the\n",
      "2656\n",
      " original\n",
      "2168\n",
      " series\n",
      "13\n",
      ".\n",
      "314\n",
      " I\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(generated_tokens):\n",
    "    \n",
    "    tok = token.item()\n",
    "\n",
    "    print(tok)\n",
    "\n",
    "    dec_tok = llm.tokenizer.decode(tok)\n",
    "\n",
    "    print(dec_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Save the logits\n",
    "        logits = llm.lm_head.output\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Get predicted token\n",
    "        pred_embed = logits.argmax(dim=-1)\n",
    "        # Decode token\n",
    "        pred_token = llm.tokenizer.decode(pred_embed).save()\n",
    "        # Calculate entropy\n",
    "        pred_entropy = nnsight.apply(calc_entropy, probs.detach().cpu()).save()\n",
    "\n",
    "        # Append values to our lists\n",
    "        entropy_values.append(pred_entropy)\n",
    "        generated_tokens.append(pred_token)\n",
    "\n",
    "        if pred_embed in end_tokens:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(314, device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time, '\n",
    "n_new_tokens = 30\n",
    "prob_values = []\n",
    "generated_tokens = []\n",
    "end_tokens = [13, 30, 0, 986]  # . ? ! ...\n",
    "\n",
    "with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    # Access the first token (works reliably)\n",
    "    with llm.lm_head.all():\n",
    "        # Process first token\n",
    "        logits = llm.lm_head.output\n",
    "        probs = F.softmax(logits, dim=-1).save()\n",
    "        pred_token = logits.argmax(dim=-1)[0][-1].save()\n",
    "        \n",
    "        # Save values\n",
    "        prob_values.append(probs)\n",
    "        generated_tokens.append(pred_token)\n",
    "    \n",
    "    # Use a while loop with careful token checks\n",
    "    token_index = 0\n",
    "    current_token = pred_token\n",
    "    \n",
    "    # Process subsequent tokens\n",
    "    while token_index < n_new_tokens - 1:\n",
    "        if current_token.item() in end_tokens:\n",
    "            # Log the stop condition\n",
    "            tracer.log(f\"Stopping at position {token_index}, token: {current_token.item()}\")\n",
    "            # Stop generation\n",
    "            llm.lm_head.output.stop()\n",
    "            break\n",
    "            \n",
    "        # Access next token\n",
    "        with llm.lm_head.next().all():\n",
    "            # Get token info\n",
    "            logits = llm.lm_head.output\n",
    "            probs = F.softmax(logits, dim=-1).save()\n",
    "            current_token = logits.argmax(dim=-1)[0][-1].save()\n",
    "            \n",
    "            # Save values\n",
    "            prob_values.append(probs)\n",
    "            generated_tokens.append(current_token)\n",
    "            \n",
    "        # Increment counter\n",
    "        token_index += 1\n",
    "\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): all input tensors must be on the same device. Received cpu and mps:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Add token to input for next iteration\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_tokens)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): all input tensors must be on the same device. Received cpu and mps:0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to calculate entropy\n",
    "def calc_entropy(probs, axis=1):\n",
    "    return entropy(probs.cpu().numpy(), axis=axis)\n",
    "\n",
    "# Prompt and setup\n",
    "prompt = \"Once upon a time, \"\n",
    "n_new_tokens = 30\n",
    "end_tokens = [13, 30, 0, 986]  # . ? ! ...\n",
    "\n",
    "# Initialize storage\n",
    "generated_tokens = []\n",
    "entropy_values = []\n",
    "\n",
    "# Tokenize the prompt\n",
    "tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate tokens one by one\n",
    "for i in range(n_new_tokens):\n",
    "    # Get logits for next token\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)[:, -1, :]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted token\n",
    "    next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    token_entropy = calc_entropy(probs)\n",
    "    \n",
    "    # Add to lists\n",
    "    generated_tokens.append(next_token.item())\n",
    "    entropy_values.append(token_entropy)\n",
    "    \n",
    "    # Check for end token\n",
    "    if next_token.item() in end_tokens:\n",
    "        print(f\"Found end token: {next_token.item()}\")\n",
    "        break\n",
    "    \n",
    "    # Add token to input for next iteration\n",
    "    tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "# Display results\n",
    "print(\"Generated tokens:\", generated_tokens)\n",
    "print(\"Token text:\", tokenizer.decode(generated_tokens))\n",
    "print(\"Entropy values:\", entropy_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
